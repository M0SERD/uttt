\section{Introduction}
%introduction of the topic
Algorithms are everywhere. They are a part of our everyday lives without even realizing it. They can be found in general mathematics, or in practical applications such as  computers or smartphones. But what exactly is an algorithm? According to the Oxford Dictionary an algorithm is generally speaken "a process or set of rules to be followed in calculations or other problem-solving operations, especially by a computer" \cite{web:oxforddictionaries2018}.
In our apprenticeship, we both have a lot to do with algorithms. Damian works as a software engineer, where he writes most of the time software. Stefan is an electronics engineer and his specialisation in the fourth year of education is also software. According to that, we both have a pretty good idea of what algorithms are and what they do. We found out, that each of us is fascinated by the concepts and possibilities algorithmic structures provide. So we came up with the idea, to implement an algorithm by ourselves. \bigskip

%goals
We set the goal, to program an algorithm, that can play the game Ultimate Tic-Tac-Toe (UTTT). UTTT is a more complex and strategic version of the ordinary Tic-Tac-Toe. 
Our approach to solve that problem is a so called Self Learning Alpha Beta Pruning Algorithm, also known as SLAP Algorithm. It's a combination of the already existing Alpha Beta Pruning algorithm combined with an own made up extension which is, as the name suggests, self learning. We want to analyse the learning process of the algorithm in more detail and evaluate whether there is really an improvement. This will be the mathematical part of our work
To experience our algorithm in action, we also wanted to create a website where everybody can play the game against our SLAP Algorithm.
Another intention of our project is to bring the seemingly complex subject in an easy understandable form to the interested reader. We aim to resume our approach of the solution in a comprehensible way. We want to give the reader an insight and a deeper understanding of algorithms, how they work and how they are implemented. With the website, we hope to create an interesting and interactive extension to this paper.
In order to meet the requirements to cover two subjects, we decided to write our paper and the website in English. \vspace{8mm}

%overview methodological approach
The first part of our IDPA will be to implement the game itself and the corresponding SLAP algorithm. 
That includes to find a way to value the states of the game in the most efficient way. This will be the task of the evolutionary algorithm. Only if that part of the SLAP algorithm works fine, the Alpha Beta Pruning is able to work in our favour.
We decided to realise all of that with a modern programming language called Dart from Google. Dart is a well-structured, object-oriented programming language that gives us the flexibility to write client and server-side code. All with one code base and one language. Once everything is set up, we will extend our project, that we are able to track the development of the self learning part. We are also planning to integrate a function into the website where the user can train his own SLAP algorithm and observe the progress themselves.
%overview structure of the paper
%to do

\section{Rules}
To understand to game Ultimate Tic-Tac-Toe, it is required to  know the rules of the ordinary Tic-Tac-Toe game. If you already know the rules of the ordinaty Tic-Tac-Toe, you can continue reading paragraph 2.2. 

\subsection {Rules Tic-Tac-Toe}
The game Tic-Tac-Toe consist of a 3-by-3 board. Two players are required while one player represents X and the the other player represents O. One player can start and put his mark anywhere he'd like to. After that, the other player can take his turn and put his mark to a remaining spot. This procedure continues until someone has 3 of his own marks vertically, horizontally or diagonal aligned. It's possible that no party wins and the match ends undecided.

\begin{fixedpic}
	\centering
	\includegraphics[width=0.2\textwidth]{tictactoe}
	\captionof{figure}{a random scenario of an ordinary Tic-Tac-Toe field}
\end{fixedpic}


\subsection {Rules Ultimate Tic-Tac-Toe}
The board of Ultimate Tic Tac Toe is made up from 9 ordinary Tic-Tac-Toe boards, hence there are 81 possible fields.
Each small Tic Tac Toe board will be called a local board and the big Tic-Tac-Toe board will be called the global board.
A player can start anywhere he'd like to on a local board. According to the location he played on a local board, his opponent will be sent to that position in the global board. The opponent can now play on the local board, keeping in mind that he will send the other player to the relative position on the global board. 

A victory of a local board is the equivalent to a marked tile on the global board. To win the game, one has to win 3 horizontally, vertically or diagonal aligned tiles of the global board.

If a local board is won or draw, no more moves are allowed there. In case a player was sent to such a board, he is allowed to make his next turn on every other local board. It's possible that the game ends in a draw, because  no more legal moves are allowed.

\begin{fixedpic}
	\centering
	\includegraphics[width=0.6\textwidth]{uttt}
	\captionof{figure}{a random scenario of an Ultimate Tic-Tac-Toe field}
\end{fixedpic}

The yellow marked local fields are already won by the yellow party.

\section{Motivation}
Since the beginning of searching a topic for our IDPA, we knew that we both were interested in a practical project rather than a  project work in written form. We both are interested in coding and have to work with software. Because of that, a software project was the most obvious option. Additionally, to create an own software project was for both of us a desirable idea.
Damian came up with the idea, to create a Self Learning Algorithm which is able to play Ultimate Tic-Tac-Toe. We both were really interested in a practical implementation of such an algorithm. We both knew that we could learn a lot with such a project and that the context of the IDPA provides the perfect opportunity for that. \\

We also knew, that we could integrate knowledge in our project, which we gained at our mathematics teaching during the last three years. To evaluate and describe certain behaviours of our algorithm, algebra and data analysis provide perfect tools.

To cover two subjects in our IDPA, we decided to apply our English knowledge, which we also improved during the last years at the BMS. Because we write all our software in English, we came up with the idea to keep language consistency throughout our whole project, including the website and this documentation. 

\section{SLAP Algorithm}

\subsection{Alpha Beta}
\begin{fixedpic}
	\centering
	\includegraphics[width=0.9\textwidth]{alphabetatree}
	\captionof{figure}{A representation of an Alpha Beta Pruning tree. \protect \cite{web:aptree} }
\end{fixedpic}



The \ac{ap}  always starts from one initial game state. This initial game state is represented by the top node. Each game state also corresponded to a number. Each game state has a certain number of moves that can be played. For example, when the game starts and the field is empty, there are 81 possible moves that can be played at a search depth of one.
All the possible moves are now tried out one after the other. After each move we get a new game state, which also has possible moves again. Even after these moves, we have new game states again. This is how one score after the other is calculated. This can be represented as a tree structure. Each node (whether round or square) symbolizes a game state, represented by a number. The branches show the possible moves. Since evaluating all scores would be too computationally intensive, a search depth is defined. In the figure the search depth would be 4, because four turns were evaluated from the initial node. Now the scores of the lowest row (hands) are evaluated. The bigger the number, the better the game state for us. And the better the game state, the sooner we will win.\\

Now the tree is evaluated so that in the worst case the score is as high as possible for us.
That means as much as assuming that the opponent always makes the move that would be worst for us, because a bad game state for us is a good game state for the opponent. This is how the evaluation starts at the bottom of the tree. The fourth turn (branches between the last two levels) is played by the opponent (MIN stands for minimizing player). Therefore, the smallest number of leaves is always written in the upper node. See the example at the bottom left:
\begin{fixedpic}
	\centering
	\includegraphics[width=0.15\textwidth]{alphabetabranch}
	\captionof{figure}{the smaller value gets written to the upper knot}
\end{fixedpic}
The parent node is given the value five, because the minimizing player always chooses the smaller value. This happens with all nodes of this level. The third move is now played by us. We'll take the highest possible score, of course. In this way, the higher-level nodes of the previously filled level are filled with the highest of the possible values. Now it's the turn of the opponent who chooses the worst score for us. The procedure repeats itself up to the initial node. In this way we get the information which move is best for us. The grey marked nodes and crossed out branches are optimizations of the Alpha Beta Pruning algorithm. So we do not have to evaluate these areas at all and can save computing time.
The search tree gets evaluated after every new turn. In other words, our Algorithm is able to calculate $n$ turns, which represents the search depth, in advance. Of course, the bigger the search depth $n$ is, the more accurate our moves can be made.

\begin{fixedpic}
	\centering
	\includegraphics[width=0.9\textwidth]{alphabetalevel}
	\captionof{figure}{all possible game moves in the 3rd level of the search depth, evaluated by the minimzing player}
\end{fixedpic}

The average amount of options per move in the game Ultimate Tic-Tac-Toe is five. \cite{web:tsurel2013}
In our case, for simplification purposes, the Alpha Beta Pruning search tree can be pictured with five branches emerging from every new node. With a search depth of n, and without the optimizations of the Alpha Beta Pruning algorithm, the calculated amounts  of all nodes  can be calculated with the following  equation:  $$\alpha = \sum_{i=1}^{n} 	\delta^{i}$$

\begin{center}
$\alpha$: amount of nodes,
$n$: search depth,
$i$: index,
$\delta$: average move per node
\end{center}


\subsection{Heuristic}
The Alpha Beta Pruning only decides which move is played. But without the heuristic, the Alpha Beta Pruning algorithm would be pointless. The task of the heuristic is to fill the nodes in the Alpha Beta Pruning search tree with meaningful numbers. The more accurate our heuristic will be, the better are the chances for our algorithm to win. The heuristic has only the purpose to evaluate the score of the board, or in other words, calculate a number for each possible game state. The better the game state, the higher the score.

Now to how the evaluation works. We award points for certain conditions. There are exactly five of these conditions. The check always takes place in a row, a column or a diagonal. The first two conditions are checked in the small fields.
\begin{itemize}
\item One field occupied, two fields empty. The condition occurs three times here. Once in the right column, once in the bottom row and once in the diagonal from top left to bottom right.
\begin{fixedpic}
	\includegraphics[width=0.2\textwidth]{con1}
	\captionof{figure}{Condition one}
\end{fixedpic}
\item Two fields occupied, one field empty. Here the condition occurs in the lowest row. Note how the first condition occurs three times again, in the left column vertically, diagonally left bottom to the right bottom and again vertically middle column.
\begin{fixedpic}
	\includegraphics[width=0.2\textwidth]{con2}
	\captionof{figure}{Condition two}
\end{fixedpic}
\end{itemize}
The following three conditions are checked in the large field.
\begin{itemize}
\item One small field won, two small fields not yet determined. Here the condition occurs once in the top row. In this figure, the first two conditions are no longer considered.
\begin{fixedpic}
	\includegraphics[width=0.2\textwidth]{con3}
	\captionof{figure}{Condition three}
\end{fixedpic}
\item Two small fields won, one small field not yet determined. Here is a visualization of this condition.
\begin{fixedpic}
	\centering
	\includegraphics[width=0.6\textwidth]{con4}
	\captionof{figure}{Condition four}
\end{fixedpic}
\item Three small squares won. This also corresponds to a victory. Here the condition occurs in the right column.
\begin{fixedpic}
	\centering
	\includegraphics[width=0.6\textwidth]{con5}
	\captionof{figure}{Condition five}
\end{fixedpic}
\end{itemize}
In the following figure, all five occurring states are marked with different colors:
\begin{description}
\item[State one] Yellow
\item[State two] Green
\item[State three] Blue
\item[State four] Purple
\item[State five] Light blue
\end{description}
\begin{fixedpic}
	\centering
	\includegraphics[width=0.6\textwidth]{allcons}
	\captionof{figure}{All Conditions}
\end{fixedpic}
The states are determined separately for both players. We take the difference of the states of the two players. That is to get a more objective result of the five game conditions. If the difference is $>0$, the first player has more occurring conditions than the second player. If the difference is equal to $0$, both player have the exact same evaluation of the corresponding condition.
If the difference is $<0$, the second player has an advantage with the corresponding condition.

After the subtraction we have five certain numbers, which describes each of the five conditions.  Now, we have to convert them into a meaningful number. This is where the DNA comes into play. The DNA is an object consisting of 5 fields. Each field stands for a factor by which the number, that represents  conditions, is multiplied. Not all conditions should contribute the same value to the final game state evaluation. With the DNA, we are able to control the relative proportions of the five conditions. Let's do a sample calculation using the picture above. Our DNA has the following Numbers.
\begin{center}
\begin{description}
\item[Factor condition one] 1
\item[Factor condition two] 3
\item[Factor condition three] 10
\item[Factor condition four] 30
\item[Factor state five] 100
\end{description}
\end{center}


In other words, the 5th DNA Value contributes 100 times more value to the final game state evaluation than the 1st DNA factor. The 4th parameter is 10 times more valuable than the 2nd parameter. In the following table, the whole calculation of the value of the game state is shown.

\begin{tabularx}{\textwidth}{|X|X|X|X|X|X|}
\hline
Condition & Occurrence Condition red & Occurrence Condition blue & Difference & DNA factor & Condition Score \\\hline
1	& 0	& 11	& -11	& 1 	& -11 \\\hline
2	& 1	& 3 	& -2	& 3 	& -6 \\\hline
3	& 4	& 0 	& 4 	& 10	& 40 \\\hline
4	& 2	& 0 	& 2 	& 30	& 60 \\\hline
5	& 1	& 0 	& 1 	& 100	& 100 \\\hline
\multicolumn{5}{|c|}{Game state score}&183 \\\hline
\end{tabularx}\\
\\
The effective game state score in this case is 183, which is all the heuristic does.
Based on this information, the Alpha Beta Pruning decides which move to play.



\subsection{Self Learning} \label{selflearning}
To say it in easy terms, the self learning part learns how to estimate the game state in the most meaningful way. It learns which heuristic parameters are important in relation to the other heuristic parameters. In our case, the DNA  is responsible for that. The DNA dictates, which heuristic parameters are more important and it does that simply with a factor. The DNA is also the only thing which changes, while learning. 

Let's take a look at how an algorithm is generated and how it gets better over time.
To start a new era, we need to set a few things up. An era can be compared to several generations of humans. 
The era contains all generations. In a human generation, we have several individuals. In our case, we will call our individuals of the generation Organisms. We have to decide, how many Organisms per generation we want to create. But we have to keep in mind, the more Organisms we create, the longer the evolution will take. That's because every Organisms will play against every other Organisms twice. They play two times, to make sure every Organisms has once the possibility to start. It's crucial, that every Organism has once the first turn, because from a statistical perspective, the player with the first move wins in 56.17\% of the cases. That is a difference of 12.34\% to the second player, which is a statistically significant advantage. \cite{web:tsurel2013}
The process of playing against each other is called selection. The Organisms get rewarded with 3 points when they win, and get 1 point when they end in a draw. When they loose, they won't get any points. The whole process of finding the best Organisms of a generation is actually very similar to a football cup.
The amount of games that are played in that way with $n$ participants is $n^2-n$.

We also need to decide the search depth of the alpha beta pruning algorithm the organisms use, while playing against each other. The bigger the search depth, the longer it will take, because the computer has to calculate more scenarios. But the deeper we search, the more accurate results we will get. 
Now after we set up the amount of organisms and the search depth, we can start our new era. The first generation gets created (initialised) with random DNA. %in welchem bereich sind die Zufallszahlen?
That means the proportion of the different parameters vary vividly. We already know, the Organisms with the best random parameter configuration will perform the best. But we don't know yet exactly, which parameters are the most important ones. To find it out, which parameters are the best ones, we now let the organisms play against each other. In other words, we will start the selection. After all matches are finished, we can rank the organisms according to the point distribution system, which we used. The worse half of our first generation will die. And as it is in real evolution, mutation will take place.
The DNA of the better half of the ranking will mutate two times and form a new generation. The best Organism of the generation will survive and continue to live in the next generation.
%eventuell genäuere/bessere beschreibung
As described at the beginning, the mutation only affects the DNA. The DNA contains the factor for each parameter and each factor will be mutated by a value between 0.8 and 1.2. This results in an almost totally new set of Organisms, which are descenders of the previous generation. The selection starts again for this generation.
This cycle will continue as long as the creator of the era desires to. \\

\begin{fixedpic}
	\centering
	\includegraphics[width=0.4\textwidth]{evoalgo}
	\captionof{figure}{visualisation of an evolutionary algorithm}
\end{fixedpic}


The era which is shown on our website contains 70 generations and each generation consists of 16 Organisms.

It took a Computer with 3.2GHz and 16 threads 8 hours to calculate everything. 14 of 16 threads were utilized. With the following equation, we can calculate the time it took for the computer to play one game:

$$ [t] =  \frac{8h \cdot 60\frac{min}{h} \cdot 60\frac{s}{min}}{16^2 - 16 \cdot 70} = 1.714s$$\\
The average duration of a game between two Organisms with above-mentioned computer is 1.714 seconds.

\subsection{Implementation in Dart}
The implementation in Dart had to be carefully planned. So we started to create the classes that store the data, like the score, for us. This step had to be carefully considered, we then built all the other classes on these classes. Afterwards, of course, we couldn't immediately deal with the algorithms and the actual meaning of the project, but had some milestones, which we could only work through one after the other, because they depended on each other.

\subsubsection{Gamecontroller}
We already have the classes that store data for us, called modal for short. But to be able to play we need a lot more information. For example, which moves can be played when a certain game state is active. Therefore we created the \texttt{GameController}, which gives us exactly such information.
\begin{lstlisting}[language=Dart,caption={Structure of the \texttt{GameController}}]
// Returns whether the game is finished or not based on the [state]
bool isGameFinished(GameState state) {
  // Implementation of IsGameFinished
  ...
}

// Return a list of all possible playable moves based on the [state]
List<Move> getAllPossibleMoves(GameState state, [State s]) {
  // Implementation of getAllPossibleMoves
  ...
}

// Reverts a played move on the [state] based on the [revertMove]
GameState revertMove(GameState state, RevertMove revertMove) {
	//Implementation of revertMove
	...
}

// Modifies the [state] based on the [move].
RevertMove playMove(GameState state, Move move) {
	//Implementation of playMove
	...
}
\end{lstlisting}
Since these functions are needed for each individual move, we have made these functions as efficient as possible. So we have already evaluated every possible Tic Tac Toe field and stored it in a cache. If we now evaluate the Ultimate Tic Tac Toe field, we can access this information and minimize the computing time per call.

\subsubsection{Game}
Now that we have all the necessary information for a gamestate, we could start implementing the game. Therefore we created classes for the game itself and for the players. Since the whole program doesn't yet run in the browser but in the console, we didn't implement any input options yet. Instead we created the simplest algorithm there is. It chooses a random playable move and plays it. Then we could let the computer play the first games. The games didn't really make sense, because only one random move was played at a time, but we reached this milestone.

\begin{lstlisting}[language=Dart,caption={Implementation of the \texttt{RandomMove}}]
class RandomMove extends Algorithm {
  Random r;

  /// Initialises a new Random Algorithm
  RandomMove() {
    r = Random();
  }

  @override
  Move getNextMove(GameState state) {
    List<Move> moves = getAllPossibleMoves(state, State.flip(state.lastMove.state));
    int randomIndex = r.nextInt(moves.length);
    return moves[randomIndex];
  }
}
\end{lstlisting}

\subsubsection{Website}
Now that we have a working game, we want to play it ourselves. That's why we started developing the website. First we had to make sure that the score was displayed correctly. Then we had to create the possibility that when we click into the grid, the correct move is played. After this implementation we were able to play against the computer, but only against the random algorithm. The further implementation of the website is described in more detail in the \autoref{websiteui}. For this milestone we only needed a playable version so that we could start implementing the Alpha Beta pruning algorithm.

\subsubsection{Heuristic}
Before we can start with the Alpha Beta Pruning algorithm, we have to implement the heuristics. The heuristic is used by the Alpha Beta Pruning algorithm. For the heuristic we first create the DNA, the base of the evaluation. Based on this DNA, the states are then added together. In order to accelerate this evaluation, we have already loaded the evaluation for all possible Tic Tac Toe fields in the background, as with the game controller. With this information we can efficiently evaluate even a large field without much computational effort. Finally, this function is called several times with each move of the algorithm.

\begin{lstlisting}[language=Dart,caption={Implementation of the evaluation}]
// Returns the evaluated score of the [state] from the
// point of view of the [primaryState]
double evaluateState(GameState state, State primaryState) {
	// Evaluates the score for the [primaryState] and
	// subtract score of the opponent
    return evaluateForState(state, primaryState)
     - evaluateForState(state, State.flip(primaryState));
}

// Returns the evaluated score of the [state] only for the [primaryState]
double evaluateForState(GameState state, State primaryState) {
    double value = 0.0;
    // Checks if the game is finished
    if(cache[state.value][primaryState].three > 0) {
      // Because the game is finished, only the last parameter of the DNA is relevant
      value += cache[state.value][primaryState].three * dna.bigThree;
    } else {
      // The game is not finished, therefore we analyse all local games
      state.tiles.map((b) => cache[b.value][primaryState]).forEach((info) {
        // If the local board is finished, we don't evaluate the score, because
        // the score will get added in the last step, where we evaluate
        // condition 3 and 4
        if(info.three == 0) {
       	  // Adds the score of condition 1 and 2 for each local board
          value += info.one * dna.smallOne;
          value += info.two * dna.smallTwo;
        }
      });
      // Finally we evaluate condition 3 and 4
      value += cache[state.value][primaryState].one * dna.bigOne;
      value += cache[state.value][primaryState].two * dna.bigTwo;
    }
    return value;
}
\end{lstlisting}

\subsubsection{Alpha Beta Pruning Algorithm}
Now we can implement the Alpha Beta Pruning algorithm. Since we already prepared everything, from the heuristics to the game controller, this work was done relatively fast. Since we also built our program cleanly with interfaces, we could simply replace the random algorithm with the Alpha Beta Pruning algorithm and play against it. However, the prerequisite for this was that the heuristic was filled with a meaningful DNA object. We created this by hand, with values that we thought made sense. Afterwards we were able to play against our algorithm. What is striking is that it was really hard to win against the algorithm, although the parameters were not defined by the computer but by us. That was of course a good sign that our approach could be successful.

\subsubsection{Selflearning Algorithm}
Since our algorithm now works, we start with the self-learning part. At first we didn't implement it on the website, but for the console. First we had to create the object that stores the whole process, the era. Then we had to implement the functions described in the \autoref{selflearning} and call them in the correct order. Then we just had to be patient to train a few generations to see if there was any improvement.

\subsubsection{Final Step}
Our main goal was achieved. The algorithm evolves itself and we can play against it. But we didn't want to be satisfied with that yet. That's why we added the self-learning algorithm to the website and added some nice features like color themes or compatibility on all devices. We also kept an eye on the performance and if possible, wrote the code as efficiently as possible.


%For dynamic parts of a website, webbrowsers such as Google Chrome or Windows Explorer only understand a language called JavaScript. Because of that, our project, %which is mainly written in Dart, gets compiled (translated) to JavaScript. In that way, we can avoid the disadvantages JavaScript has.

\subsection{Website and User Interface} \label{websiteui}
In order that our project is accessible all over the world with different end devices ranging from Smartphones to Personal Computers running with different Operating Systems, we thought that it would be the most convenient solution to make a website. 

In general, the static part of the website is made with HTML (Hyper Text Markup Language) and CSS (Cascaded Style Sheets). HTML specifies the structure of a website.  CSS is then required to style everything. The dynamic part was already discussed in the previous subsection. 
Because CSS can be extremly time consuming, we made use of a CSS Framework called Materialize from Google, which provides cross browser compatible and responsible components such as navbars, buttons, dropdown-lists, cards, modals and many more. To make our website more appealing and fluid, we used CSS animations. For this purpose, we used the website Animista, which provides ready-to-use CSS animations.

To deploy our website online, we also needed to have a hosting service which offers a server and a domain. Because we already worked with tools of Google like the language Dart and the Materialize Framework, it wasn't far-fetched, that we decided to make use of a hosting provider which also belongs to Google. It's called Firebase. Firebase provides many options for developers such as Real Time Databases or Cloud Messaging, but we only needed to use the hosting opportunity, which supports hosting static files like CSS and HTML.\\

Our website consists of two main parts. The first part, we called it "Reduced View", is mainly designed for an easy game experience, where you can choose between three levels to play against the algorithm. The levels include "easy", "medium" and "hard". Behind the easy level plays the best organism of the first generation. 
The medium level is played by the best organism of the 35th %evtl korrektur
generation and the hard level is the best organism of the whole era. The default search depth of the current game is three. Three is an experiential value, which is the best compromise between search duration and reliability of the algorithm.
The first part of the website contains an explanation of the rules of the game as well.

Via a button in the navigation bar, it's possible to change to a more detailed  part of the website, we called it "Advanced View", which we designed for people who are interested in what's going on behind the scenes. It's possible to see the whole era with all generations and organisms and it's evident, how the algorithm has evolved. It's also possible to generate a new era and calculate everything from scratch. As one plays against a desired Organism, it's also possible to adjust the search depth of the current game dynamically. It's observable, that it takes much longer for the algorithm to make a turn if the search depth is deeper. %evtl zeit erwähnen wie lange es geht

%evtl weglassen oder verbessern
Our Algorithm is separated from the part of the Web, that it is modular and reusable for any other applications like an app or something similar. In programming terms, things are separated with a thing called "Interface". Our Interface in dart is called the Player Interfacer. The Player Interface simply provides the information of our game state and is able to get input from the user interface on our website.

In HTML and CSS, we prepared a 9x9 grid, which we had to fill dynamically with the information of our Player Interface. When the algorithm made his turn, we also implementet a function that shows the player, where he has to play his next turn. 
We also had ho make sure, that the calculations of the algorithm didn't run on the same thread as the UI thread of the website. Otherwise, the UI couldn't be updated at the moment when the user clicked, which would be a bad user experience.


\subsection{Runtime}
Our two algorithms both depend on different parameters. These influence the runtime in different ways. If they are too small, we do not get an optimal result; if they are too large, the runtime can be too long.

\subsubsection{Alpha Beta Pruning algorithm}
The Alpha Beta Pruning algorithm has several factors that influence it. One factor is the average search depth, here $\delta$. This factor is always similar for our game, but the more advanced the score, the smaller it gets. The second factor is the search depth, here $n$. The deeper you search, the more scores you have to evaluate. This factor influences the score exponentially. Therefore, the search depth of the era must also be carefully selected when creating. So we have a runtime of $\delta^n$ which in the O-notation, which is used in computer science, corresponds to a runtime of $O(N^{m})$. This means that the parameter $n$ influences the runtime exponentially.

\subsubsection{Self Learning algorithm}
The self-learning algorithm has two parameters. One parameter is the number of organisms per generation, $g$ for short. Since all play against each other, with outward and return play, formula for the number of games is $g^2 - g$.  The second parameter is the search depth with which the organisms play against each other. Now we run the Alpha Beta Pruning Algorithm once for each move per game with our search depth $n$. Since we don't know the average number of moves per game, we use the variable $\omega$.  And we already know the runtime of the Alpha Beta Pruning algorithm. So in the end we have a runtime of $(g^2 - g) * \omega * delta^n$. The first part $(g^2 - g) * \omega$, which is influenced by the parameter $g$, has a runtime of $O(N^2)$. The second part, which represents the Alpha Beta Pruning algorithm, is $O(N^k)$ as shown above. This means that the size of the generation affects the runtime quadratically, but the search depth exponentially. So the search depth should be played around more cautiously, as with the size of the generations. 

\subsubsection{Device Restrictions}
We have made our program as device friendly as possible. So you don't need a powerful computer to play games, a smartphone is enough. To train the generations, on the other hand, as much computing power as possible is needed. So you will get better results with a multicore PC than with a smartphone. Since the software runs on as many threads as possible, this is the main criterion of the training. So it was also surprising that a game on my smartphone, the Galaxy S8, was played faster than on my business PC. 
As we already found out in \autoref{selflearning}, on a computer with 16 threads we needed an average of $[t] = 1,714s$. If we multiply that by the 14 ($maximale\ number\ threads - 2$) working threads, we get $[t] = 24 s$.
If we now calculate the time for the smartphone we get the following result. $ [t] = \frac{1.5min \cdot 60\frac{s}{min}}{(6^2 - 6)} = 3s$. If we multiply this with the 6 working threads we get $[t] = 3s \cdot 6 = 18s$. This means that the smartphone actually works faster than the computer, but would still be slower because the computer has 8 threads more to work with. So you can't generally say that a device is unsuitable for training organisms. However, the more threads and speed a device has, the faster the organisms can be trained.

\section{Progress Evaluation}
As already mentioned, first-generation organisms are produced from randomly generated DNA. Thus, on average, all parameters are still evaluated in approximately the same way. However, this changes over the generations.

\subsection{Expected Result}
According to instinct we can say that condition 5, a victory of the game, should be rated better than condition 1. We can also say that condition 2 should be rated better than condition 1, because with condition 2 we are closer to winning a local game. Also a won local game, condition 3, is better than a nearly won local game, condition 2.
The point is that the conditions are already sorted the way we expect them to be weighted. Condition 1 should have the least weight, condition 5 the most.

\subsection{Received results}
The first generation consists only of randomly created organisms. Nevertheless, a first development can already be observed. The best organism already shows more or less the expected curve. The following states are better evaluated than the previous ones. This probably also helped him to victory. Also the last organism is the opposite. With the exception of the last parameter, the following states are rated worse than the previous ones.
\begin{fixedpic}
\begin{dnadiagram}
\addplot coordinates{(1, 108) (2, 379) (3, 856) (4, 1648) (5, 1745)};
\addplot coordinates{(1, 1408) (2, 1575) (3, 1801) (4, 205) (5, 1508)};
\addplot coordinates{(1, 1586) (2, 790) (3, 494) (4, 439) (5, 697)};
\legend{First of Genration 1,Median of Generation 1,Last of Generation 1}
\end{dnadiagram}
\captionof{figure}{The best, an average and the last DNA of Generation 1}
\end{fixedpic}
While in the first generation only the first organism more or less reproduced the expected curve, in the 5th generation there are already 11 of 16 organisms with such DNA. The remaining 5 organisms show the curve up to the fourth parameter, but the fifth parameter was mutated badly and the curve goes down again. So we have in only five generations already a first trend which plays itself in and already strongly differs from the generation 1.
\begin{fixedpic}
\begin{dnadiagram}
\addplot coordinates{(1, 112) (2, 398) (3, 735) (4, 1410) (5, 1890)};
\addplot coordinates{(1, 122) (2, 362) (3, 750) (4, 1280) (5, 1646)};
\addplot coordinates{(1, 118)(2, 423)(3, 1078)(4, 1507)(5, 1123)};
\legend{First of Genration 5,Median of Generation 5,Last of Generation 5}
\end{dnadiagram}
\captionof{figure}{The best, an average and the last DNA of Generation 5}
\end{fixedpic}
In the tenth generation there is again a conspicuity. Slowly all organisms start to look the same, but places 3 and 4 stand out. In these organisms, the fourth and fifth parameters are almost identical. Probably because the fourth parameter mutated strongly upwards. Although these organisms do not look like the expected result, they are better than many that look like the expected result. The striking difference to the others is also that the relative difference of conditions 3 and 4 is greater than for the others. Precisely because the fourth parameter has probably mutated strongly upwards. According to this observation, the ratio of the parameters plays an important role.

\begin{fixedpic}
\begin{dnadiagram}
\addplot coordinates{(1, 99) (2, 555) (3, 730) (4, 1294) (5, 2055)};
\addplot coordinates{(1, 60) (2, 324) (3, 642) (4, 1664) (5, 1763)};
\addplot coordinates{(1, 92) (2, 281) (3, 796) (4, 2018) (5, 2055)};
\legend{First of Genration 10,Third of Generation 10,Fourth of Generation 10}
\end{dnadiagram}
\captionof{figure}{Some conspicuous DNA of Generation 10}
\end{fixedpic}

From the tenth generation onwards, these conditions are further developed. So the best of the tenth generation reminds rather of a line, whereas the best of the generation 20 reminds rather of a (half-)parabola. This development continues until generation 70, where we have completed the development.

\begin{fixedpic}
\begin{dnadiagram}
\addplot coordinates{(1, 99) (2, 555) (3, 730) (4, 1294) (5, 2055)};
\addplot coordinates{(1, 103) (2, 369) (3, 1110) (4, 2209) (5, 3033)};
\addplot coordinates{(1 ,74) (2 ,275) (3 ,732) (4 ,1387) (5 ,3976)};
\addplot coordinates{(1, 66) (2, 282) (3, 583) (4, 1451) (5, 5091)};
\addplot coordinates{(1, 61) (2, 327) (3, 717) (4, 1542) (5, 5683)};
\addplot coordinates{(1, 71) (2, 368) (3, 476) (4, 1800) (5, 8907)};
\legend{First of Generation 10,F.o.G. 15,F.o.G. 20, F.o.G. 30, F.o.G. 50, F.o.G. 70}
\end{dnadiagram}
\captionof{figure}{First}
\end{fixedpic}

\todo{Nach State und Condition suchen und alles aufräumen}

\subsection{Why we chose Dart}
At the beginning of every project there is always the question which programming language to use. Since we didn't know exactly how we wanted to implement the program, we decided on a versatile language. Dart runs in the browser, respectively you can compile it in Javascript, and also runs on the server side and if necessary we could even build the code into an app. The only, just as versatile language and alternative would be Javascript. However, Dart was designed to compete with Javascript.  The main goal was to eliminate the errors and inconveniences of Javascript. So Dart offers an excellent asynchronous language support, which is unfortunately missing in Javascript. Also the language is easier to read, because it is data type based and has many small features, which make the code more compact. Also, we both have no experience with dart yet, what we would like to have changed.  And in this project we both saw our chance to change that. All these little things let us decide for Dart. The following is a small example of what is meant by the small features that make the code more compact.

\begin{lstlisting}[language=JavaScript,caption={Example in JavaScript}]
// Returns obj2 if obj1 is null, otherwise returns obj1
function method1(obj1, obj2) {
	if(obj1 == null) {
		return obj2;
	} else {
		return obj1;
	}
}

// Executes the method bar, but only if foo is not null
function method2(foo) {
	if(foo != null) {
		foo.bar();
	}
}
\end{lstlisting}

\begin{lstlisting}[language=Dart,caption={Example in Dart}]
// Same as above but in Dart
dynamic method1(dynamic obj1, dynamic obj2) {
	return obj1 ?? obj2;
}

// Same as above but in Dart
void method2(Foo foo) {
	foo?.bar();
}
\end{lstlisting}

\section{Challenges}
\subsection{Worker}

\subsection{Responsive design with CSS}
When designing a website, it's important to consider, that different end devices are used to visit the website. The screen size and ratio of a normal desktop computer is extremely different from a smartphone. In order to render the website well on all end devices, \ac{responsive} design is required. 

To meet that requirement, fortunately we had the Materialize CSS frame work. Despite that, it wasn't always easy to achieve a seamless design on different end devices. While we developed the website, some components were often displaced, when we accessed the unfinished website with different end devices. 
In the end, even tough it was challenging, we had a well structured responsive design.

\section{Review}
In retrospect, it was a very interesting project to create. It was very motivating to us, to see the project grow while achieving small goals. Despite the fact that the algorithm works in theory, we had our concerns that it would seamlessly work in a practical implementation. It was a relieve, when we both saw, that the self learning part of the algorithm evolved in accordance to our expectations.
